{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMBDWizTvrcGZt5F5hwYV3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirishbahirat/Artificial-Intelligence-1/blob/master/rf-basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btqYzTlutedZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "661f48f0-582f-42bb-ac6b-af70856947d0"
      },
      "source": [
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('NChain-v0')\n",
        "env.reset()\n",
        "\n",
        "\n",
        "def naive_sum_reward_agent(env, num_episodes=500):\n",
        "    # this is the table that will hold our summated rewards for\n",
        "    # each action in each state\n",
        "    r_table = np.zeros((5, 2))\n",
        "    for g in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.sum(r_table[s, :]) == 0:\n",
        "                # make a random selection of actions\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                # select the action with highest cummulative reward\n",
        "                a = np.argmax(r_table[s, :])\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            r_table[s, a] += r\n",
        "            s = new_s\n",
        "    return r_table\n",
        "\n",
        "\n",
        "def q_learning_with_table(env, num_episodes=500):\n",
        "    q_table = np.zeros((5, 2))\n",
        "    y = 0.95\n",
        "    lr = 0.8\n",
        "    for i in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.sum(q_table[s, :]) == 0:\n",
        "                # make a random selection of actions\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                # select the action with largest q value in state s\n",
        "                a = np.argmax(q_table[s, :])\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
        "            s = new_s\n",
        "    return q_table\n",
        "\n",
        "\n",
        "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
        "    q_table = np.zeros((5, 2))\n",
        "    y = 0.95\n",
        "    eps = 0.5\n",
        "    lr = 0.8\n",
        "    decay_factor = 0.999\n",
        "    for i in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        eps *= decay_factor\n",
        "        done = False\n",
        "        while not done:\n",
        "            # select the action with highest cummulative reward\n",
        "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                a = np.argmax(q_table[s, :])\n",
        "            # pdb.set_trace()\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
        "            s = new_s\n",
        "    return q_table\n",
        "\n",
        "\n",
        "def run_game(table, env):\n",
        "    s = env.reset()\n",
        "    tot_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        a = np.argmax(table[s, :])\n",
        "        s, r, done, _ = env.step(a)\n",
        "        tot_reward += r\n",
        "    return tot_reward\n",
        "\n",
        "\n",
        "def test_methods(env, num_iterations=100):\n",
        "    winner = np.zeros((3,))\n",
        "    for g in range(num_iterations):\n",
        "        m0_table = naive_sum_reward_agent(env, 500)\n",
        "        m1_table = q_learning_with_table(env, 500)\n",
        "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
        "        m0 = run_game(m0_table, env)\n",
        "        m1 = run_game(m1_table, env)\n",
        "        m2 = run_game(m2_table, env)\n",
        "        w = np.argmax(np.array([m0, m1, m2]))\n",
        "        winner[w] += 1\n",
        "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
        "    return winner\n",
        "\n",
        "\n",
        "winner = test_methods(env, 500)\n",
        "print(winner)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Game 1 of 500\n",
            "Game 2 of 500\n",
            "Game 3 of 500\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}